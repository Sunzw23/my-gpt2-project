\section{Sun Zhongwei: Training and evaluating a mini-GPT model}

This part documents the process of training and evaluating a mini-GPT model for generating text in the style of William Shakespeare. The model was implemented using PyTorch and trained from scratch without pretrained transformer weights. We experimented with different architectures and hyperparameters to understand their impact on model performance.

\begin{center}
    \fbox{\parbox{0.9\textwidth}{
        \centering
        \textbf{Code Repository} \\
        \url{https://github.com/Pinarinith/my-gpt2-project}
    }}
\end{center}

\subsection{Dataset Preparation}
The dataset used was \texttt{tinyshakespeare.txt} containing Shakespeare's works. The data was processed as follows:

\begin{itemize}
    \item Character-level tokenization was performed
    \item The dataset was split into:
    \begin{itemize}
        \item Training set: 80\% of the data
        \item Validation set: 10\% of the data
        \item Test set: 10\% of the data
    \end{itemize}
    \item Context length (block size) was set to 128 characters
\end{itemize}

The vocabulary size was determined to be \texttt{65} unique characters after processing.

\subsection{Model Architecture}
The mini-GPT model follows the standard Transformer architecture with the following components:

\begin{itemize}
    \item Token embeddings layer
    \item Positional embeddings
    \item Multiple transformer layers with self-attention
    \item Feed-forward networks
    \item Final linear projection to vocabulary size
\end{itemize}

\subsection{Training Procedure}
We experimented with multiple configurations as specified in the \texttt{config.json} file. Two representative configurations are highlighted below:

\subsubsection*{Configuration 1: Smaller Model}
\begin{itemize}
    \item Layers: 4
    \item Attention heads: 4
    \item Embedding dimension: 256
    \item Dropout: 0.1
    \item Learning rate: 3e-4
\end{itemize}

\subsubsection*{Configuration 2: Larger Model}
\begin{itemize}
    \item Layers: 8
    \item Attention heads: 8
    \item Embedding dimension: 512
    \item Dropout: 0.1
    \item Learning rate: 1e-4
\end{itemize}

Training was performed with:
\begin{itemize}
    \item Batch size: 64
    \item Epochs: 5
    \item Optimizer: AdamW
    \item Evaluation interval: Half of training set size
\end{itemize}

\subsection{Results}

The results of the two configurations are shown below, for more details see the training log on the repository website.

\subsubsection*{Configuration 1: Smaller Model}

The model architecture used the following parameters:
\begin{itemize}
    \item Transformer Layers: 4
    \item Attention Heads: 4
    \item Embedding Dimension: 256
    \item Dropout Rate: 0.1
    \item Learning Rate: 0.0003
\end{itemize}

\subsubsection*{Training Results}
The model achieved optimal performance after 20 epochs:
\begin{itemize}
    \item Best Validation Loss: 0.295
    \item Final Batch: 13,940
\end{itemize}

\subsubsection*{Test Performance}
The model was evaluated on a held-out test set with the following results:

\begin{table}[h]
\centering
\caption{Test Performance Metrics}
\label{tab:metrics}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Average Loss & 0.2950 \\
Token Accuracy & 91.45\% \\
Perplexity & 1.34 \\
Vocabulary Diversity & 0.3484 \\
Sequence Accuracy & 91.32\% \\
Exact Match Rate & 0.00\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection*{Generated Text Samples}
The model was tested with different generation parameters:

\subsubsection*{Sample 1: Temperature=0.8, Top-k=50}
\begin{verbatim}
HAMLET: To be or not to be talk'd for, to accept it.

KING RICHARD II:
And buried, gentle Tyrrel?

TYRREL:
The grey-office with such a deceitful soul
That hath two them kings murder'd:
But they shall set up forward towards on their market-place;
Call me to the master?

Page:
No, no, no, no; for she hath praised his master.

TRANIO:
Sir, how now, sir! What's the matter? My dear inclinest maid
That wash'd my hands with thy sword sweet wives and with
Rainous with silver savage and talk of perpetual
To greet the traitor's na
\end{verbatim}

\subsubsection*{Sample 2: Temperature=0.5, Top-k=50}
\begin{verbatim}
HAMLET: To be or not to be endured!

GLOUCESTER:
I hope the king is not here pass'd for so fair?
She may year more, may betide to my suit;
And so, here comes Katharina, thy supper is:
Where is the best of Katharina must be.

SAMPSON:
The better for his father's death, and I will speak:
There is no more but strange.

Shepherd:
I cannot speak, Northumberland, I crave it,
When I shall not do it.

CLARENCE:
Then this is the duke my father, cry 'Come;'
O, 'alce on father, but not who came.

First Murderer:
Ay, stay we now no 
\end{verbatim}

\subsubsection*{Sample 3: Temperature=0.8, Top-k=0 (No restriction)}
\begin{verbatim}
HAMLET: To be or not to be so resolved
As vaults should be thine. To have heard your person;
'Tis more, thanks, my gracious lord.

DUKE VINCENTIO:

ISABELLA:

DUKE VINCENTIO:
Let's hear. O sir, ha! Away with him!

LUCIO:
A hundred moulderous leisure; the worst is full of
meat, and here be absent: it may be denied but my
believe in Vienna, is the grave of men alive?

PETRUCHIO:
It was an errand.

HORTENSIO:
Sir, fairly dream!

GREMIO:
Horse! a devil, a red son, or an apparent,
Of such post surfeits that we have always bef
\end{verbatim}

\subsubsection*{Analysis}
Key observations from the experiment:
\begin{itemize}
    \item The model achieved excellent token accuracy (91.45\%) while maintaining low perplexity (1.34)
    \item Higher temperature (0.8) produced more creative but less coherent text
    \item Lower temperature (0.5) resulted in more conservative but grammatically sound output
    \item Disabling Top-k sampling led to slightly less coherent generations
    \item The zero exact match rate confirms the model isn't simply memorizing text
\end{itemize}

Here is another example in a similar format, simulating results from a different model configuration:

\subsubsection*{Configuration 2: Medium Model}

The model architecture used the following parameters:
\begin{itemize}
    \item Transformer Layers: 8
    \item Attention Heads: 8
    \item Embedding Dimension: 512
    \item Dropout Rate: 0.1
    \item Learning Rate: 0.0001
\end{itemize}

\subsubsection*{Training Results}
The model achieved optimal performance after 25 epochs:
\begin{itemize}
    \item Best Validation Loss: 0.100
    \item Final Batch: 17,425
\end{itemize}

\subsubsection*{Test Performance}
The model was evaluated on a held-out test set with the following results:

\begin{table}[h]
\centering
\caption{Test Performance Metrics}
\label{tab:metrics_medium}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Average Loss & 0.1001 \\
Token Accuracy & 96.74\% \\
Perplexity & 1.11 \\
Vocabulary Diversity & 0.3766 \\
Sequence Accuracy & 96.46\% \\
Exact Match Rate & 0.00\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection*{Generated Text Samples}
The model was tested with different generation parameters:

\subsubsection*{Sample 1: Temperature=0.8, Top-k=50}
\begin{verbatim}
HAMLET: To be or not to be spoke withal.
Here comes his servant: how now, Catesby,
What says he?

CATESBY:
My lord: he doth entreat your grace;
To visit him to-morrow or next day:
He is within, with two right reverend fathers,
Divinely bent to meditation;
And no worldly suit would he be moved,
To draw him from his holy exercise.

BUCKINGHAM:
Return, good Catesby, to thy lord again;
Tell him, myself, the mayor and citizens,
In deep designs and matters of great moment,
No less importing than our general good,
Are come to ha
\end{verbatim}

\subsubsection*{Sample 2: Temperature=0.5, Top-k=50}
\begin{verbatim}
HAMLET: To be or not to be spoke withal.
Here comes his servant: how now, Catesby,
What says he?

CATESBY:
My lord: he doth entreat your grace;
To visit him to-morrow or next day:
He is within, with two right reverend fathers,
Divinely bent to meditation;
And no worldly suit would he be moved,
To draw him from his holy exercise.

BUCKINGHAM:
Return, good Catesby, to thy lord again;
Tell him, myself, the mayor and citizens,
In deep designs and matters of great moment,
No less importing than our general good,
Are come to ha
\end{verbatim}

\subsubsection*{Sample 3: Temperature=0.8, Top-k=0 (No restriction)}
\begin{verbatim}
HAMLET: To be or not to be talked on, you shall bear more.

CORIOLANUS:
The gods begin to mock me. I, that now
Refused most princely gifts, am bound to beg
Of my lord general.

COMINIUS:
Take't; 'tis yours. What is't?

CORIOLANUS:
I sometime lay here in Corioli
At a poor man's house; he used me kindly:
He cried to me; I saw him prisoner;
But then Aufidius was within my view,
And wrath o'erwhelm'd my pity: I request you
To give my poor host freedom.

COMINIUS:
O, well begg'd!
Were he the butcher of my son, he should
Be fre
\end{verbatim}

\subsubsection*{Analysis}
Key observations from the experiment:
\begin{itemize}
    \item The model showed improved performance over the smaller variant with a token accuracy of 96.74\% and perplexity of 1.11
    \item At temperature 0.8, the model generated rich and contextually appropriate text while maintaining coherence
    \item Lower temperature settings produced more conservative but still stylistically consistent output
    \item Vocabulary diversity increased slightly compared to the smaller model, indicating better use of language variation
    \item Despite high sequence accuracy, the zero exact match rate suggests strong generalization rather than memorization
    \item Generated samples demonstrated understanding of Shakespearean style and structure
\end{itemize}


\subsection{Conclusion}
We successfully trained and evaluated mini-GPT models for Shakespearean text generation. The larger model with 8 layers and 512 embedding dimension performed better but at the cost of increased computational requirements. Future work could explore more sophisticated sampling techniques and larger context windows.

